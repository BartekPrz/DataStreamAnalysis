\chapter{Wstęp: Uczenie maszynowe w analizie strumieni danych}

\noindent W dzisiejszym świecie użytkownicy internetu przyzwyczajeni są do gromadzenia i udostępniania danych w dowolnym miejscu oraz o dowolnym czasie. Wraz ze wzrostem liczby ludzi posiadających dostęp do sieci oraz rozwojem technologii automatycznego gromadzenia i przechowywania informacji można zaobserwować wzrost rozmiarów przechowywanych danych. Zgromadzone zbiory mogą być szczególnie interesujące dla osób zajmujących się eksploracją danych, ze względu na możliwość odkrycia interesującej oraz wartościowej wiedzy reprezentowanej przez ukryte wzorce. Wyzwaniem w tym przypadku jest nie tylko efektywne przechowywanie danych, lecz także ich analiza, zdolność interpretacji i wyciągania użytecznych wniosków, które mogą prowadzić do lepszych decyzji \cite{DBrzezinski}\cite{Prezentacja:ZED}.

Według raportu \cite{Article:BigData} w samym roku 2021 na świecie wygenerowano około 79 zettabajtów danych. Szacunki na następne lata pokazują, że w roku 2022 może to być około 94 ZB, podczas gdy w 2025 roku liczba ta może osiągnąć nawet wartość ponad 150 ZB. Jednym z czynników wpływających na ogólną liczbę przetwarzanych informacji w dzisiejszym świecie są aplikacje, w których dane są generowane z bardzo dużą szybkością w postaci nieustannie zmieniających się \textit{strumieni danych}. Jako przykłady aplikacji generujących strumienie danych można wyszczególnić, np. systemy rekomendacyjne, systemy nawigacyjne w sztucznej inteligencji, systemy odpowiedzialne za badanie opinii w czasie rzeczywistym czy systemy odpowiedzialne za monitorowanie sieci komputerowych, telekomunikacyjnych i transakcji bankowych \cite{DBrzezinski:Prezentacja}.

Wspomniane wcześniej pojęcie \textit{strumienia danych} można opisać jako sekwencję elementów, które napływają w sposób ciągły w zmiennych interwałach czasu. Ze względu na ilość oraz szybkość napływania nowych przykładów eksploracja strumieni danych wymusza na badaczach uwzględnienie pewnych ograniczeń takich jak: ograniczony czas oraz ograniczona pamięć. Wspomniane ograniczenia sprawiają, że w środowisku strumieniowym nie jest możliwe przechowywanie w pamięci wszystkich przykładów, które napłynęły od początku działania danego algorytmu \cite{DBrzezinski}\cite{BrzezPhd2015}.

W standardowym ujęciu statycznego środowiska systemów uczących się, zbiór danych jest niezmienny oraz dostępny dla badacza przez cały czas. Przetwarzany zbiór danych może zostać poddany wielu modyfikacjom, każdy przykład ze zbioru może zostać przetworzony więcej niż jeden raz. Ze względu na wcześniej wspomniane ograniczenia: nieograniczoność zbioru danych (brak możliwości zapamiętania całego strumienia), krytyczny czas przetwarzania i odpowiedzi, jednorazowy odczyt elementu danych oraz możliwą zmienność rozkładów danych w czasie (\english{concept drift}) nie jest możliwe zastosowanie statycznych algorytmów eksploracji danych w środowisku strumieniowym \cite{Article:TradDataStream}\cite{Prezentacja:Strumienie}.

Zjawisko dryfu pojęć (\english{conecpt drift}), które można opisać jako zmiany definicji klas przewidywanych przez model w czasie, znacząco przyczyniło się do powstania nowych metod eksploracji strumieni, które radziłyby sobie ze zmiennością rozkładów danych. Do wspomnianych metod można zaliczyć okna przesuwne (\english{sliding windows}), detektory dryfu (\english{drift detectors}) czy modelowanie klasyfikatorów złożonych (\english{ensemble classifiers}) \cite{DBrzezinski}.

Ostatnimi czasy powstało wiele algorytmów podejmujących temat radzenia sobie ze zmiennością definicji klas przewidywanych przez model predykcyjny w czasie \cite{BrzezPhd2015}\cite{Article:ManyAlgorithms}\cite{Article:OBFirst}\cite{Article:OBSecond}. W pracach poświęconych konkretnym algorytmom do przetwarzania strumieni danych pokazano, że nowe metody przetwarzania są w stanie radzić sobie z niezbalansowanymi i zmiennymi strumieniami danych, ale tylko dla określonych scenariuszy. Wielu badaczy w swoich pracach jako zmianę w rozkładzie danych analizowali głównie zmianę globalnego współczynnika niezbalansowania między klasami (\english{imbalance ratio}). W artykule \cite{Article:TypyPrzykladow} zwrócono szczególną uwagę na dodatkowe czynniki, które poza zmianą liczności występowania przykładów z danych klas, wpływają na ogólną ocenę procesu klasyfikacji. Efektem końcowym pracy było zaprezentowanie tego, jak radzą sobie wybrane algorytmy w przypadku przetwarzania strumieni danych charakteryzujących się określonymi zmianami jak, np. podział klasy mniejszościowej na kilka mniejszych grup czy też napływ przykładów z klasy mniejszościowej określonego typu trudności. Szczególną uwagę zwrócono na analizę najbliższego sąsiedztwa danego przypadku, co pozwoliło na ostateczne określenie jego typu trudności jako przypadek bezpieczny, brzegowy, rzadki lub odstający (przyjęta konwencja została szerzej opisana w sekcji \ref{Section:DriftDataDistribution} niniejszej pracy). Wiele algorytmów radziło sobie bardzo dobrze na strumieniach charakteryzujących się występowaniem pojedynczego czynnika trudności w danych, jednak trafność predykcji znacząco spadała w przypadku, gdy czynników trudności w analizowanych danych było więcej.

Kierując się powyższymi motywacjami w niniejszej rozprawie podjęto próbę rozszerzenia istniejących algorytmów ukierunkowanych na przetwarzanie strumieni danych w taki sposób, aby stworzone algorytmy były w stanie poradzić sobie z klasyfikacją strumieni danych charakteryzujących się czynnikami trudności opisanymi w pracy \cite{Article:TypyPrzykladow}.