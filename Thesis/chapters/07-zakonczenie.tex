\chapter{Uwagi końcowe}

\noindent Jednym z powszechnych zjawisk dnia codziennego w dzisiejszym świecie jest generowanie dużych ilości danych przez użytkowników internetu. Wraz ze wzrostem liczby ludzi posiadających dostęp do sieci oraz rozwojem technologii automatycznego gromadzenia i przechowywania informacji można zaobserwować wzrost rozmiarów przechowywanych danych. W samym roku 2021 na świecie wygenerowało około 79 ZB danych. Jednym z czynników wpływających na ogólną liczbę przetwarzanych informacji są aplikacje, w których dane generowane są z bardzo dużą szybkością w postaci nieustannie zmieniających się strumieni danych. Wygenerowane sekwencje danych stały się przedmiotem badań, których celem było powstanie nowych typów algorytmów - klasyfikatorów strumieniowych.

Celem niniejszej pracy było zaproponowanie nowych algorytmów przetwarzania strumieniowego, które poradziłyby sobie z nauką na niezbalansowanych i zmiennych strumieniach danych. Do tej pory w literaturze najczęściej analizowaną zmianą w strumieniu była zmiana globalnego współczynnika niezbalansowania (\english{global imbalance ratio}). Kilku badaczy w swoich pracach wykazało jednak, że inne czynniki takie jak np. podział grupy przykładów z klasy mniejszościowej na kilka mniejszych grup czy napływ przypadków określonego typu, mogą być znacznie bardziej wpływowe na jakość klasyfikacji aniżeli zmiana współczynnika niezbalansowania. Wobec tego faktu stworzone algorytmy zostały przetestowane na wygenerowanych strumieniach danych z określonym typem zmiany w celu określenia jak dany klasyfikator radzi sobie z określonym typem zmiany.

W ramach pracy zaproponowano trzy nowe algorytmy odpowiedzalne za klasyfikację niezbalansowanych i zmiennych strumieni danych. Pierwsze z podejść to algorytm \textit{Neighbourhood Oversampling Online Bagging}, którego główną ideą jest modyfikacja parametru $\lambda$ rozkładu Poissona dla przykładów z klasy mniejszościowej. Przy wyznaczaniu tego parametru brane pod uwagę są dwa czynniki: liczność przykładów w każdej z klas w danym momencie w czasie oraz analiza najbliższego sąsiedztwa dla określonego przykładu. Drugim z podejść był algorytm \textit{Neighbourhood Undersampling Online Bagging}, którego idea jest bardzo podobna jak w przypadku poprzednika, z tą różnicą, że modyfikacja parametru $\lambda$ rozkładu Poissona dokonywana jest dla przykładów z klasy większościowej. Ostatnim z podejść jest algorytm \textit{Hybrid Neighbourhood Online Bagging}, który w swoim działaniu wykorzystuje jako klasyfikatory składowe algorytmy \textit{NOOB} oraz \textit{NUOB}. Predykcja dla tego podejścia odbywa się według algorytmu, który w danej chwili ma wyższą wartość miary \textit{G-mean}.

W dalszej części przeprowadzono ocenę eksperymentalną dla klasyfikatorów opisanych w poprzednim akapicie oraz dla algorytmów znanych z literatury, takich jak: \textit{Online Bagging}, \textit{Oversampling Online Bagging}, \textit{Undersampling Online Bagging}. Przeprowadzona analiza wykazała, że nowe podejścia \textit{NUOB} oraz \textit{NOOB} radzą sobie szczególnie dobrze dla złożonych strumieni danych, które posiadają przynajmniej dwa czynniki trudności. Stworzone na podstawie testów statystycznych rankingi pokazały, że wyniki generowane przez zaproponowane algorytmy są statystycznie istotne w porównaniu do pozostałych algorytmów. Podobną sytuację można zaobserwować w przypadku analizy podejścia hybrydowego, które łączy zalety dwóch wcześniej zaproponowanych podejść. Przeprowadzone testy statystyczne dla algorytmu hybrydowego wykazały, że różnica pomiędzy tym podejściem a resztą algorytmów jest statystycznie istotna. W przypadku analizy strumieni danych z jednym czynnikiem trudności wszystkie podejścia osiągały bardzo podobne wyniki, tylko dla niektórych typów zmian możliwe było wskazanie algorytmu statystycznie istotnego od pozostałych.

Mimo że zaproponowane podejścia charakteryzują się poprawą jakości klasyfikacji pod kątem analizowanych miar \textit{G-mean} oraz \textit{Recall}, to nadal widoczne jest miejsce do poprawy aktualnie osiąganych wyników. Szczególnie największy spadek jakości klasyfikacji widoczny jest dla strumieni danych zawierających w swoim dryfie napływ przykładów typu \textit{Rare}. Obserwacja ta otwiera pole do działania dla przyszłych badaczy zajmujących się tematyką niezbalansowanych i zmiennych strumieni danych.

Jedną z modyfikacji, która mogłaby przynieść poprawę wyników, a której nie zdołano przetestować w niniejszej pracy, jest skorzystanie z algorytmu bazującego na detektorze dryfu, np. \textit{DDM} lub \textit{EDDM} wraz z wprowadzeniem buforów odpowiedzialnych za przechowywanie określonych typów przykładów. W ten sposób, poprzez analizę najbliższego sąsiedztwa danego elementu, byłoby możliwe zebranie przykładów trudnych do nauki, które następnie mogłyby być wykorzystane do stworzenia nowego klasyfikatora po przekroczeniu poziomu alarmu przez dotychczasowy model. Podejście to pozwoliłoby na szersze spojrzenie pod kątem analizy określonych typów przykładów przez algorytmy przetwarzania strumieniowego.